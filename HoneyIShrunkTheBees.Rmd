---
title: "Honey I Shrunk the Bees!"
author: "Chris G Martin"
date: "May 15, 2016"
output:
  html_document:
    toc: TRUE
    toc_float: TRUE
    highlight: pygments
    theme: cerulean
  pdf_document: default
---

```{r include=FALSE, cache=FALSE}
# DO NOT REMOVE
# THIS IS FOR SETTING SOME PLOTTING PARAMETERS SO THAT YOUR PLOTS DON'T TAKE UP TOO MUCH SPACE
# IF YOU WOULD LIKE TO CHANGE THESE, SEE HELP FILES ON THE par() FUNCTION
# OR ASK FOR HELP
library(knitr)
## set global chunk options
opts_chunk$set(fig.path='figure/manual-', cache.path='cache/manual-', fig.align='center', fig.show='hold', par=TRUE)
## tune details of base graphics (http://yihui.name/knitr/hooks)
knit_hooks$set(par=function(before, options, envir){
if (before && options$fig.show!='none') par(mar=c(4,4,.2,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)
})
```


## Part 1 - Introduction:

Bees: the most **effective** pollinator on the planet. In the United States, we seem to have a love/hate relationship with these facinating creatures; on the one hand we are well aware and appreciative of their role in nature and yet the number of bees is quickly dwindling without us giving any significant help. But, one of the many questions we will explore here is: *do we rely on them?*

By looking at a variety of data, we hope to find links between the production of honey and other agricultural statistics.

### Why Honey:

Collecting honey is an ancient tradition dating back over 8,000 years ago$^1$. It's a product made by bees while foraging for nectar from flowers. While honey bees are the most common type of bee that produces honey that humans extract for food, there are a number of other bees and wasps that create honey, and of course all bees and other insects pollinate. For the purposes of this project, we are going to ignore the details and make the assumption that honey is directly related to bees.

Many/most/all beekeepers harvest honey from their bees as a source of income, but they also sell or rent hives to farms. As the number of bees across the world have steadily dropped (for several reasons, none-of-which we will explore here), renting or selling hives has become a lucrative business. The sold or leased bees are taken to a farm and help to naturally pollinate farms and the lands around them, helping the farmer to grow crop, cultivate land, fertilize land, and develop the land more effectively. Since the bees return to the hive after foraging, they make honey from the nectar, are returned the beekeeper or farmer, who in turn can harvest the honey.

Honey production is valuable to the United States economy. Honey is used in making desserts, breads, barbecues, mustards, jellies and ointment treatments. Honey as amazing health benefits, too. It's loaded with antibacterial and antifungal properties and has been known to work well as a dextromethorphan, which is a common ingredient in cough medications to soothe cough. Additionally, honey can treat dandruff, used in energy drinks, and treat wounds and burns. It is a common belief that honey consumption can also help fight local allergies.

This study sets out to understand better understand some of the impacts on honey production in the United States by exploring various data fields across all 50 states, and futher inbetween.

## Part 2 - Data:

Outlined in this section is the data processes: [1] Importing, [2] Data Exploration, [3] Data manipulation (cleaning and tidying), and [4] Data Structuring. Please see [Appendix 1: Data Merging in MySQL](https://github.com/chrisgmartin/HoneyIShrunkTheBees/tree/master/Appendix) for more details on the data importing and manipulation techinques; the MySQL queries can also be found there. All of the data has been saved in the [GitHub folder: Data](https://github.com/chrisgmartin/HoneyIShrunkTheBees/tree/master/Data).

### Data Import:

The most important part of this analysis is obviously the data. Fortunately there is a plethora of open source data available from a variety of sources.

After following some detailed [Stack Overflow](https://stackoverflow.com/questions/10292326/how-to-connect-r-with-mysql-or-how-to-install-rmysql-package) tips, I was able to connect to a local MySQL server that I created called 'HISP' (Honey, I Shrunk the Production). Please see the HIPS database sources in Appendix 1 and outlined below.

```{r}
library(RODBC)
con <- odbcConnect("hisp")
sqlQuery(con, "use hisp")
```

#### National Agriculture Statistics Service:

The first set of data came from the [National Agriculture Statistics Service website](https://quickstats.nass.usda.gov/) (NASS), which houses a large amount of data related to United States Agriculture. The website stores census and survey data on a number of subjects (the main categories are animals, products, crops, demographics, economics, and environment) across a number of geographic levels (national, state, county, and zip code) for a number of years. The data can be access by the website UI allowing the user to select each subject, geographic level, and year which can then be exported into a .CSV file, or the entire database can be downloaded as a text file after extracting the database .GZ. Howevey, it was simple enough to query the database using the website rather than have to deal with the large and cumbersome dataset. The queries were then saved into seperate .CSV files, [imported into MySQL, and prepped](https://github.com/chrisgmartin/HoneyIShrunkTheBees/blob/master/HoneyIShrunkTheBees.sql). It is now ready to import into R:

```{r}
#load the data
honey_county <- as.data.frame(sqlQuery(con, "select * from honey_county", stringsAsFactors=FALSE))
honey_state <- as.data.frame(sqlQuery(con, "select * from honey_state", stringsAsFactors=FALSE))
#If MySQL isn't set up for you, the tables can be imported via GitHub:
#honey_county <- read.csv('https://raw.githubusercontent.com/chrisgmartin/HoneyIShrunkTheBees/master/Data/honey_county.csv', header = TRUE)
#honey_state <- read.csv('https://raw.githubusercontent.com/chrisgmartin/HoneyIShrunkTheBees/master/Data/honey_state.csv', header = TRUE)


kable(head(honey_county, 3))
kable(head(honey_state, 3))
```


#### American National Standards Institute (ANSI) Codes

The NASS data sets include a column for ANSI codes at both the state and county level. These ANSI codes can be extremely useful in analyzing our data, especially when it comes to plotting it all on a map. ANSI stands for [American National Standards Institute](https://www.census.gov/geo/reference/ansi.html) which are standardized codes used to ensure uniform identification of geography between various agencies. Since September 2008, ANSI has also replaced the use of [Federial Information Processessing Standard](https://en.wikipedia.org/wiki/FIPS_county_code) (FIPS) codes and National Institute of Standards and Technology codes (NIST). 

One item of note is that the states in our NASS data sets are full text state names (for example, California instead of CA, or New York instead of NY). There is a seperate SQL script hosted at [statetable.com](http://www.statetable.com)$^2$ that was used to convert the full length text in the NASS data into its abbreviated form to simplify our useage. The script is hosted on my [GitHub page](https://github.com/chrisgmartin/HoneyIShrunkTheBees/blob/master/Data/state_table.sql) for reference.

It might also come in handy to save the the FIPS County codes$^3$ [reference table](https://www.census.gov/geo/reference/codes/cou.html), which has been saved and stored locally using MySQL, in case we need to reference it:

```{r}
#load FIPS 
FIPS <- (sqlQuery(con, "select * from FIPS", stringsAsFactors=FALSE))

#For those without a MySQL link established:
#FIPS <- read.csv('https://raw.githubusercontent.com/chrisgmartin/HoneyIShrunkTheBees/master/Data/FIPS.csv', header = TRUE)
```

#### Closing the MySQL Connection:

For safe measures, we should always close out our MySQL connection to maintain data integrity.

```{r}
odbcClose(con)
```

### Data Cleaning

We have a number of items here to sort through, but need to clean what can be cleaned. Other items of note are the columns. There are several that are unnecessary for our purposes. Essentially, the columns we need (or might need) are: Program (column 1), Year (2), GeoLevel (5), State (6), StateANSI (7), County (10), CountyANSI (11), Commodity (16), DataItem (17), and Value (20). The first 10 items are descriptive, while the final column contains our variable. At the state level, we'll also need some addidtional columns Domain (18) and DomainCategory (19) that we'll group by later to remove.

Let's pull in only the columns we need:

```{r}
honey_county2 <- honey_county[c(1,2,5,6,7,10,11,16,17,20)]
kable(head(honey_county2))
honey_state2 <- honey_state[c(1,2,5,6,7,10,11,16,17,18,19,20)]
kable(head(honey_state2))
```

Futher exploration of the data found several characters in the Value column, we'll have to fix that but need to first understand why the characters are there. Lucky for us, the NASS website also includes a [glossary](https://quickstats.nass.usda.gov/src/glossary.pdf) which is more than handy in a situation like this. The values we see here are: (D) and (Z), which mean "Withheald to avoid discolsing data for individual operations" and "Less than half of the rounding unit" respectively. As I understand it (Z) is essentially a (NULL) value and can be simply replaced with a 0 but (D) is tricky since it could be 0 or it could be extremely large. There are a number of things we could do to fix them: replace them with 0 (positively skewing our results), use a total average (skewing our results in a strange way), use a weighted average based on a certain metric (skewing our data), average based on another metric (also skewing our data), or make an assumption (which will most likely skew our data). What I think from my gut instinct is that using an average based on a metric would be the best way to do this as it is less prone to futher skewing our data in any extreme cases. The metric that I think will be the most effective is an average for the state for the specific data item; an alternative at the county level would be to use average by state/county/data item, but having tried that unsuccessfully it led to creating too many null values due to some statistics not being available at that level. Thankfully, SQL makes doing just that really simple (see the first appendix--even though I'll include the code below for some of the R work). Let's try to put that together:


```{r}
#Replace (Z)'s and (D)'s with 0 value
#honey_county2$Value[honey_county2$Value == " (Z)"] <- as.integer(0)
#honey_state2$Value[honey_state2$Value == " (Z)"] <- as.integer(0)
#honey_county2$Value[honey_county2$Value == " (D)"] <- as.integer(0)
#honey_state2$Value[honey_state2$Value == " (D)"] <- as.integer(0)

#Remove comma's from digits
library(stringr)
#honey_county2$Value <- str_replace_all(honey_county2$Value, "[:punct:]", "")
#honey_state2$Value <- str_replace_all(honey_state2$Value, "[:punct:]", "")

#check to see if it worked correctly
#honey_county2$Value[honey_county2$Value == " (Z)"]
#honey_county2$Value[honey_county2$Value == " (D)"]
#honey_state2$Value[honey_county2$Value == " (Z)"]
#honey_state2$Value[honey_county2$Value == " (D)"]
```

*Important note*: R currently cannot properly store integer values greater than 2,147,483,647 (I was getting an error which [pointed me to this link](http://stackoverflow.com/questions/14589354/struggling-with-integers-maximum-integer-size) ). As a result, I have fixed the "*CROP TOTALS - SALES, MEASURED IN $*" rows in the county file (which contained several values greater than 2,147,483,647) to be the log form, and 66 other rows in the state file. The original idea was to change all items above this limit to equal the limit but it would absolutely lead to errors in our data, and in a real-life situation this would never be a suggestion to recommened, ever. In real-life, we will always be affected by the assumptions we make, so limiting the potential for data bias can go a long way. The solution was to set all values in the State dataset as its log value which also has the benefit of 'normalizing' the data.

```{r}
#Change Values into Int
honey_county2$Value <- as.integer(honey_county2$Value)
honey_state2$Value <- as.integer(honey_state2$Value)

#Remove NA's to zero
#NA's are from null values from the MySQL Query
#Where (D) values did not have an average to calculate
honey_county2$Value[is.na(honey_county2$Value)] <- as.integer(0)
honey_state2$Value[is.na(honey_state2$Value)] <- as.integer(0)
```

But we're not done yet! ANSI codes need to be in a specific order for mapping. They need a 3 digit length with leading zeros. NULL values are present! But removing rows with NULL values in CountyANSI shouldn't handled in the same way as before (we used 0's for when Values were NULL) because then we would be losing essential information. Rather, we can leave them as NULL because the CountryANSI column is only used in plotting values on a map; the values still exist and remain in the data, but since we can't decide where to place them on the map they remain invisible. Then we're faced with another task: mapping using the ChoroplethR package requires combined 2 digit state ANSI codes AND 3 digit county codes. I've included the code to covert them below,  however the 'maps' package requires only a combined State ANSI code (1 or 2 digit) and County ANSI code (3 digit), so we'll only keep those active, and the State requirement is the State name uncapitalized which will be fixed as needed.

```{r}
#honey_county2$StateANSI2 <- sprintf("%02d", honey_county2$StateANSI)
#honey_state2$StateANSI2 <- sprintf("%02d", honey_state2$StateANSI)
honey_county2$CountyANSI <- sprintf("%03d", honey_county2$CountyANSI)
honey_county2$CountyANSI <- as.numeric(paste0(honey_county2$StateANSI, honey_county2$CountyANSI))
```

### Data Tidying

With the data loaded, and more-or-less cleaned, for viewing purposes it should be tidy'd a bit.

```{r}
honey_county2 <- honey_county2 %>% 
  arrange(., DataItem) %>% 
  arrange(., State)

honey_state2 <- honey_state2 %>% 
  arrange(., DataItem) %>% 
  arrange(., State)
```

Another arrangement we will need is to re-format our table. Our table currently exists with each data item and each year each per state and/or each county having it's own row and value, but we actually need each data item to have it's own column so that a (for example) Alaska will have one row per year.

```{r}
#install.packages('tidyr')
library(tidyr)

honey_county3 <- honey_county2[, c(2,4,5,6,7,9,10)] %>% 
  spread(., DataItem, Value)
kable(head(honey_county3, 1))

#grouping needs to be applied at this level
#because there are the additional columns for
#Domain and DomainCategory
honey_state2_tmp <- honey_state2 %>% 
  group_by(Year, State, DataItem) %>% 
  summarise(Value = sum(Value))

honey_state3 <- honey_state2_tmp %>% 
  spread(., DataItem, Value)
kable(head(honey_state3, 1))
```

Now, what do we do with pesky null values? We cleaned it up before, back when each data item had its own row, but now that the data items are rolled-up into their State/Year combinations we may have some rows where our data on the row to be analysed--**HONEY - PRODUCTION, MEASURED IN LB** for counties, and **HONEY - PRODUCTION, MEASURED IN LB / COLONY** for state level data--is null. Let's remove those data items, and change all null values (again) to zero.

```{r}
#example with NA columns
head(honey_county3$`HONEY - PRODUCTION, MEASURED IN LB`)

honey_county3 <- honey_county3[!(is.na(honey_county3$`HONEY - PRODUCTION, MEASURED IN LB`)),]
head(honey_county3$`HONEY - PRODUCTION, MEASURED IN LB`)

honey_state3 <- honey_state3[!(is.na(honey_state3$`HONEY - PRODUCTION, MEASURED IN LB / COLONY`)),]
```



### Data Exploration

Since we have the data set we can explore what's in it before we decide where and how to clean it. Let's start with the column for data descriptions: Data Item

```{r}
length(unique(honey_county2$DataItem))
nrow(honey_county2)
length(unique(honey_state2$DataItem))
nrow(honey_state2)

```

There are a lot more data items (87 more to be exact) in the state set than in the county set, yet the county set has more variables/row (33,548).

```{r}
kable(unique(honey_county$DataItem))
```

#### Data Check: Exploring Bee Colonies

At this stage, it would be extremely helpful to check that our data is, in fact, useful. The reason we do this here is that we need to know whether we can move on from the *Data* stage and into the *Exploration* phase which will lead us into our *Inference* and *Conclusion* stages. Once this box is checked, we can move on.

Let's now take a look at how the number of honey bee colonies are disbursed in the states (remember, all values in the States data set are log values):

```{r}
#calculate or plot number of bee colonies over the years
kable(honey_county2[which(honey_county2$DataItem == 'HONEY, BEE COLONIES - OPERATIONS WITH SALES'), c(2,9,10)] %>% 
  group_by(Year) %>% 
  summarise(value = sum(Value)))

kable(honey_state2[which(honey_state2$DataItem == 'HONEY, BEE COLONIES - OPERATIONS WITH SALES'), c(2,9,12)] %>% 
  group_by(Year) %>% 
  summarise(value = sum(exp(Value))))
```

Interestingly, the number of bee colonies (those that are actually operations with sales, not wild bees) didn't change a whole lot between 2002 and 2007 at the county level; it's more difficult to tell at the state level though as they have dropped significantly from 1997 and 2002 but grew between 2002 and 2007.

#### Data Check: Looking into Honey Production

In the Honey Production data, there are a number of variables that we can choose to analyse such as the number of Operations with Sales, number of Operations with Production, Sales in $'s, and Production in lb's. The most important for this project I believe is Production in lb's since the number of operations can vary (due to business consolidation, expansion, etc.) and sales is highly influenced by economic factors not just how the bee's "performed" throughout the year.

```{r}
kable(honey_county2[which(honey_county2$DataItem == 'HONEY - PRODUCTION, MEASURED IN LB'), c(2,9,10)] %>% 
  group_by(Year) %>% 
  summarise(value = sum(Value)))

kable(honey_state2[which(honey_state2$DataItem == 'HONEY - PRODUCTION, MEASURED IN LB'), c(2,9,12)] %>% 
  group_by(Year) %>% 
  summarise(value = sum(exp(Value))))
```

Again, the production of honey seems to follow the size of the bee colonies with its ups and downs through the years. Also very facinating, we can see how the honey production per colony has changed at the state level:

```{r}
kable(honey_state2[which(honey_state2$DataItem == 'HONEY - PRODUCTION, MEASURED IN LB / COLONY'), c(2,9,12)] %>% 
  group_by(Year) %>% 
  summarise(value = sum(exp(Value))))
```

Extremely interesting. We're seeing a major drop in production per colony throughout the years.

### Initial Observations and Comments:

The resources I listed (mainly NASS), are excellent resources and economically valueable resources for researchers and businesses alike. The data seems to be well maintained, fairly simple to extract, and reliably updated. Kudos, for that.

Of the variables selected there are some I expect to have high positive correlation to Honey Production, while some I expect to have a strong negative correlation with Honey Production. Of those, horticulture data seems like it would have a strong impact as bees rely on pollen to survive. Chemical and Fertilizer use, however, I expect to have a strong negative correlation. My initial opinion is that these chemicals and fertilizers make it difficult for bees to sustain themselves in the "nature" we force them into. Crops, I believe, can be a "control" group in that I expect them to be fairly neutral to our Honey Produciton as they may or may not be directly related to our pollinator end product.

Another observation is the limited data in some categories. For example, at the state level for Honey Production in lb's, we have annual data from 1987 to 2015; however, in the county level we only have three years of data: 2002, 2007, and 2012. This limits our ability to really dive into the data.

By the end of this analysis: 1I don't expect my personal expectations to bias my end results, in fact I hope I am pleasantly surprised my results in the end.

## Part 3 - Exploratory data analysis:

Here we will try to explore some of our data and see if we can find any interesting details that would be helpful to know.

### Map

Perfect for our needs, the [ChoroplethR package](https://cran.r-project.org/web/packages/choroplethr/index.html) can help us visualize our data directly onto a map of the United States (it can do others countries too, but we only have data on the United States). The package can let us visualize at the state and county level, and has a some extra reference tables which could be useful such as the **state regions** table matching our (now cleaned up version of the) data:

```{r}
library(choroplethr)
library(choroplethrMaps)
data(state.regions)
kable(head(state.regions))
```

Let's take a look at how the number of honey bee colony operations with sales are distributed across the United States:

```{r}
HS_BeeCol <- honey_state2[which(honey_state2$DataItem == 'HONEY, BEE COLONIES - OPERATIONS WITH SALES'), c(4,12)]

HS_HonPlot <- HS_BeeCol %>% 
  group_by(State) %>% 
  summarize(value = sum(Value))

HS_HonPlot <- data.frame(region= state.regions$region, value= HS_HonPlot[match(state.regions$abb, HS_HonPlot$State), 2])

state_choropleth(HS_HonPlot, title = "Honey Bee Colony Operations", legend = "Num of Colonies", num_colors = 1)
```

Texas seems to have the highest number of bee colony operations, with California, Pennsylvania, and North Carolina following behing.

As far as counties go, the map is a bit more segregated than expected with some counties having large quantities of bee colonies than others (black counties have no colonies). FOr example, most of California bee colony operations are in Southern California, around San Diego. Nevada and the central United States seems to have a lack of any honey bee operations, and Central Florida has a massive quantity.

```{r}
HC_BeeCol <- honey_county2[which(honey_county2$DataItem == 'HONEY, BEE COLONIES - OPERATIONS WITH SALES'), c(7,10)]

colnames(HC_BeeCol) <- c('region', 'value')

HC_HonPlot <- HC_BeeCol %>% 
  group_by(region) %>% 
  summarize(value = sum(value))

county_choropleth(HC_HonPlot, title = "Honey Bee Colony Operations", legend = "Num of Colonies", num_colors = 1)
```

We can also look to see how much Honey Bee Production per Colony varies between states:

```{r}
HS_HoneyProdPer <- honey_state2[which(honey_state2$DataItem == 'HONEY - PRODUCTION, MEASURED IN LB / COLONY'), c(4,12)]

HS_HonProdPlot <- HS_HoneyProdPer %>% 
  group_by(State) %>% 
  summarize(value = sum(Value))

HS_HonProdPlot <- data.frame(region= state.regions$region, value= HS_HonProdPlot[match(state.regions$abb, HS_HonProdPlot$State), 2])

state_choropleth(HS_HonPlot, title = "Honey Bee Production per Colony", legend = "lb's of Honey", num_colors = 1)
```

It seems like there is a fairly even amount of stability in this, with the exception of Nevada (whose bees apparently are less productive than others) and Texas (whose bees apparently believe in the Texas motto: "Bigger Is Better").

Let's also take a look at chemical expenditures:

```{r}
HS_ChemExp <- honey_state2[which(honey_state2$DataItem == 'CHEMICAL TOTALS - EXPENSE, MEASURED IN $'), c(4,12)]

HS_ChemPlot <- HS_ChemExp %>% 
  group_by(State) %>% 
  summarize(value = sum(Value))

HS_ChemPlot <- data.frame(region= state.regions$region, value= HS_ChemPlot[match(state.regions$abb, HS_ChemPlot$State), 2])

state_choropleth(HS_ChemPlot, title = "Chemical Expenditures", legend = "Amount of Chemical Expenses in $", num_colors = 1)
```

At the State level, expenditures on chemicals are surprisingly high. Details on what these chemicals are, unfortunately, are not here at the moment.

```{r}
HC_ChemExp <- honey_county2[which(honey_county2$DataItem == 'CHEMICAL TOTALS - EXPENSE, MEASURED IN $'), c(7,10)]

colnames(HC_ChemExp) <- c('region', 'value')

HC_ChemPlot <- HC_ChemExp %>% 
  group_by(region) %>% 
  summarize(value = sum(value))

county_choropleth(HC_ChemPlot, title = "Chemical Expenditures", legend = "Amount of Chemical Expenses in $", num_colors = 1)
```

At the county level, almost all of California's expenditures are made right in the farming belt of Central California which is no surprise. What is surprising though is the concentration of expenditures compared to the rest of the United States. Some further research could be done here, as expenditures may mean where the expense was booked and not where the chemicals were distributed or used.


```{r}
HS_FertExp <- honey_state2[which(honey_state2$DataItem == 'FERTILIZER TOTALS, INCL LIME & SOIL CONDITIONERS - EXPENSE, MEASURED IN $'), c(4,12)]

HS_FertPlot <- HS_FertExp %>% 
  group_by(State) %>% 
  summarize(value = sum(Value))

HS_FertPlot <- data.frame(region= state.regions$region, value= HS_FertPlot[match(state.regions$abb, HS_FertPlot$State), 2])

state_choropleth(HS_FertPlot, title = "Fertilizer Expenditures", legend = "Amount of Fertilizer Expenses in $", num_colors = 1)
```

Fertilizer expenditures (including Lime and Soil Conditioners) looks similar to the chemicals map, but Texas is expending more on fertilizers comparitively than they were on chemicals. At the county level, same story as Chemicals with such a large expense taking place in the Central California region.

```{r}
HC_FertExp <- honey_county2[which(honey_county2$DataItem == 'FERTILIZER TOTALS, INCL LIME & SOIL CONDITIONERS - EXPENSE, MEASURED IN $'), c(7,10)]

colnames(HC_FertExp) <- c('region', 'value')

HC_FertPlot <- HC_FertExp %>% 
  group_by(region) %>% 
  summarize(value = sum(value))

county_choropleth(HC_FertPlot, title = "Fertilizer Expenditures", legend = "Amount of Fertilizer Expenses in $", num_colors = 1)
```

Horticulture sales are highly concentrated on the West Coast and Florida:

```{r}
HS_HortSales <- honey_state2[which(honey_state2$DataItem == 'HORTICULTURE TOTALS - SALES, MEASURED IN $'), c(4,12)]

HS_HortPlot <- HS_HortSales %>% 
  group_by(State) %>% 
  summarize(value = sum(Value))

HS_HortPlot <- data.frame(region= state.regions$region, value= HS_HortPlot[match(state.regions$abb, HS_HortPlot$State), 2])

state_choropleth(HS_HortPlot, title = "Horticulture Sales", legend = "Amount of Sales of Horticulture in $", num_colors = 1)
```

But for our purposes what could be important in horticulture is the number of acres in production, rather than sales, and it seems like there is a more equal distribution of horticulture acres in production across the United States, with Calfornia and Florida having the lions share. I would venture to guess there is some cross over between horticulture production and chemical/fertilizer use, but that's another topic from another day.

```{r}
HS_HortProd <- honey_state2[which(honey_state2$DataItem == 'HORTICULTURE TOTALS, (EXCL CUT TREES), IN THE OPEN - ACRES IN PRODUCTION'), c(4,12)]

HS_HortPlot2 <- HS_HortProd %>% 
  group_by(State) %>% 
  summarize(value = sum(Value))

HS_HortPlot2 <- data.frame(region= state.regions$region, value= HS_HortPlot2[match(state.regions$abb, HS_HortPlot2$State), 2])

state_choropleth(HS_HortPlot2, title = "Acres of Open Horticulture In Production", legend = "Number of Acres", num_colors = 1)
```

At the county level, it appears as more acreage of horticulture in California is open in Southern California (vs Central California's farming belt), with some in the North West US and a bright blue dot in the center of Tennesee. There's also a peculiar lack of acreage straight down the middle of the United States.

```{r}
HC_HortProd <- honey_county2[which(honey_county2$DataItem == 'HORTICULTURE TOTALS, (EXCL CUT TREES), IN THE OPEN - ACRES IN PRODUCTION'), c(7,10)]

colnames(HC_HortProd) <- c('region', 'value')

HC_HortPlot <- HC_HortProd %>% 
  group_by(region) %>% 
  summarize(value = sum(value))

county_choropleth(HC_HortPlot, title = "Acres of Open Horticulture In Production", legend = "Number of Acres", num_colors = 1)
```

### Data Summary

Just to recap, so far we've managed to pull in our data from the NASS website, clean it (remove some character values, set others to a state average amount, remove punctuation, set values to integers, and set our state level numbers to their log form), organize it to our needs (sort by state), tidy'd it so that each data item has its own column, did some futher cleaning (removed columns with no values for **HONEY - PRODUCTION, MEASURED IN LB** for counties, and **HONEY - PRODUCTION, MEASURED IN LB / COLONY** for state level data) and explored some interesting visualizations for the data. We saw that the number of honey bees has been dynamic in that the number of colonies decreased and increased, however the amount of honey per colony we've produced has decreased significantly over the years. As we're not performing a full time-series analysis on this data, we should simply keep this trend in mind for our conculsions. We also see that some states have more expenditures in chemicals and fertilizers than others, and we've seen that there are plenty of acres of open horticulture in production for our wonderful bees to pollinate.

So let's move on. The fun's over, it's been great, but let's get back to business. How does all of this data come together? Is it possible to create a linear model to predict future honey production?



## Part 4 - Inference:

Theoretical inference (if possible) - hypothesis test and confidence interval
Simulation based inference - hypothesis test and confidence interval
Brief description of methodology that reflects your conceptual understanding

NEED TO DESCRIBE WHAT I WILL BE DOING IN THIS SECTION



#### Summary Statistics

Let's first check out some summary statistics on our selected data: "HONEY - PRODUCTION, MEASURED IN LB" or "HONEY - PRODUCTION, MEASURED IN LB / COLONY".

```{r}
summary(honey_county3$'HONEY - PRODUCTION, MEASURED IN LB')
summary(honey_state3$'HONEY - PRODUCTION, MEASURED IN LB')
summary(honey_state3$'HONEY - PRODUCTION, MEASURED IN LB / COLONY')
```

Our county level data has definite outliers and skew, given the spread between the 3rd quartile and the maximum integer, as well as the spread in the median and mean. Same with our state level data (remember, it's already in log form). Let's see how honey production per colony appears:

```{r}
summary(honey_state3$'HONEY - PRODUCTION, MEASURED IN LB / COLONY')
```

Much more normal in appearance, but that would be assumed since all values are in log form. A histogram confirms both results:

```{r}
hist(honey_state3$'HONEY - PRODUCTION, MEASURED IN LB / COLONY')
hist(honey_county3$'HONEY - PRODUCTION, MEASURED IN LB')
```

As we also can assume from our previous exploration of the data, there are many more observation variables in our county set than in our state set:

```{r}
length(honey_county3$'HONEY - PRODUCTION, MEASURED IN LB')
length(honey_state3$'HONEY - PRODUCTION, MEASURED IN LB / COLONY')
```

With the length of our data, we can use this to create sample sizes useful for inference. Let's use a random sample size of 60 (randomly selected size).

#### Determining the Status Quo: Expected Honey Production per Colony

One question we can begin to answer at this stage is: What should the "typical" amount of honey produced per colony be? Since we only have data on this category in the state set, we will focus our attention there. Also, we now know that our full data population has 1273 variables, and we've decided to use a random sample of 60. With this as our requirement, we can set up some of the ground work:

```{r}
population <- honey_state3$'HONEY - PRODUCTION, MEASURED IN LB / COLONY'
samp_mean <- rep(NA, 50)
samp_sd <- rep(NA, 50)
n <- 60
```

These boundaries are simply pulling our population (1273 variables of lb's of Honey Produced per Colony), and determining that we will have 50 test cases with 60 observations per each case. We can then use these to loop through and build 50 random samples of 60 observations, at the same time calculating the mean and standard deviation of the sample groups and storing them into their own vectors for **samp_mean** and **samp_sd** respectively:

```{r}
for(i in 1:50){
  samp <- sample(population, n)
  samp_mean[i] <- mean(samp)
  samp_sd[i] <- sd(samp)
}
```

With our sample groups built and means / standard deviations calculated we can see what a 95% confidence interval would be for the amount of honey produced per colony:

```{r}
lower_vector <- samp_mean - 1.96 * samp_sd / sqrt(n) 
upper_vector <- samp_mean + 1.96 * samp_sd / sqrt(n)

c(lower_vector[1], upper_vector[1])
mean(population)
```

As we can see, our population average falls right inside the inveral.


#### Inference






## Part 5 - Conclusion: 















## References:

[1]  Crane, Eva (1983) The Archaeology of Beekeeping, Cornell University Press, ISBN 0-8014-1609-4

[3] statetable.com; http://www.statetable.com

[2] FIPS County COdes; https://www.census.gov/geo/reference/codes/cou.html


## Appendix (optional):

Appendix 1: Data Merging in MySQL: [MySQL Query](https://github.com/chrisgmartin/HoneyIShrunkTheBees/blob/master/HoneyIShrunkTheBees.sql) and [Appendix 1 RMarkdown write-up](https://github.com/chrisgmartin/HoneyIShrunkTheBees/blob/master/Appendix/Appendix1-DataMySQL.Rmd)

## Special Thanks

